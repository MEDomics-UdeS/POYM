"""
Filename: elstm_base_models.py

Authors: Hakima Laribi

Description: This file is used to define the ELSTM model

"""
from typing import List, Optional

import numpy as np
import torch

from src.data.processing.datasets import HOMRDataset
from src.models.abstract_models.base_models import BinaryClassifier


class EnsembleLSTMBinaryClassifier:
    """
    Class that defines the ELSTM. This Ensemble model combines the predictions of multiple LSTMs to make a prediction
    """

    def __init__(self,
                 pretrained_models: List[BinaryClassifier]
                 ):
        """
        Sets public, protected and private attributes

        Args:
            pretrained_models: list of all the pretrained models

        """

        # Sets protected attributes
        self._pretrained_models = pretrained_models

    def fit(self,
            dataset: HOMRDataset):
        """
        No fitting is needed as the models averages only the predictions of multiple LSTMs
        """

        pass

    def get_single_predictions(self,
                               dataset: HOMRDataset,
                               mask: List[List[int]]):
        """
        Gets the predicted probabilities by each model for each individual in the mask

        Args:
            dataset: HOMR dataset
            mask : indexes masks
        """
        x_probas = []

        x, y, _ = dataset[mask]
        relative_indexes, _ = dataset.flatten_indexes(mask)  # get the last index of each temporal sequence
        # Get probabilities generated by each pretrained model
        for pretrained_model in self._pretrained_models:
            probas = pretrained_model.predict_proba(dataset, mask).cpu().detach().numpy()[relative_indexes]
            x_probas.append(probas)

        x_probas = np.vstack(x_probas).T

        # Flatten y and get the last index of each temporal sequence
        y_probas = torch.flatten(torch.cat(y)).cpu().detach().numpy()[relative_indexes]

        return x_probas, y_probas

    def predict_proba(self,
                      dataset: HOMRDataset,
                      mask: Optional[List[List[int]]] = None) -> np.array:
        """
        Computes final probabilities using the Super Learner

        Args:
            dataset: HOMR dataset
            mask: dictionary with masks indexes
        """
        # Get mask
        mask = mask if mask is not None else dataset.test_mask

        original_x, _, idx = dataset[mask]
        n_models = len(self._pretrained_models)

        # 1- Create bins of patients having the same history size
        original_positions = {size: [] for size in range(1, len(self._pretrained_models) + 1)}
        x_bins = {size: [] for size in range(1, len(self._pretrained_models) + 1)}
        idx_bins = {size: [] for size in range(1, len(self._pretrained_models) + 1)}
        for indice, index in enumerate(idx):
            x_row = original_x[indice]
            size = x_row.shape[0] if x_row.shape[0] <= len(self._pretrained_models) else len(self._pretrained_models)
            x_bins[size].append(x_row)
            idx_bins[size].append(index)
            original_positions[size].append(indice)

        # 2- Make the predictions
        pred = np.empty(0, dtype=np.float32)
        for bin_size, x in x_bins.items():
            if len(x) > 0:
                x_probas = []
                relative_indexes, _ = dataset.flatten_indexes(idx_bins[bin_size])
                if bin_size < n_models:  # Case where we vectorize everything
                    x = torch.stack(x)

                    # Ensemble prediction
                    for pretrained_lstm in self._pretrained_models[(bin_size-1):]:
                        probas = self.predict_from_lstm(x, pretrained_lstm._model)[relative_indexes]
                        x_probas.append(probas)

                else:
                    # Prediction with the last model only
                    pretrained_lstm = self._pretrained_models[-1]
                    probas = self.predict_from_lstm(x, pretrained_lstm._model)[relative_indexes]
                    x_probas.append(probas)

                # Convert to numpy array
                x_probas = np.vstack(x_probas).T

                # Add the predictions of the current bin to the list of predictions
                pred = np.concatenate((pred, np.mean(x_probas, axis=1)), axis=0)

        # 3- Reorgnize the predictions
        idx_positions = []
        for positions in original_positions.values():
            idx_positions += positions
        pred_position = list(zip(pred, idx_positions))
        # Sort the list of tuples based on the original positions
        pred_position.sort(key=lambda x: x[1])

        # Extract the sorted order of elements from list A
        reorganized_pred = np.array([prediction_position[0] for prediction_position in pred_position])
        return reorganized_pred

    @staticmethod
    def predict_from_lstm(x,
                          pretrainedlstm):
        # Set model for evaluation
        pretrainedlstm.eval()

        # Execute a forward pass and apply a sigmoid
        with torch.no_grad():
            if isinstance(x, list):
                return torch.cat([torch.sigmoid(pretrainedlstm(x[l].unsqueeze(dim=0))).cpu() for l in range(len(x))],
                                 dim=0).squeeze()
            else:
                return torch.sigmoid(pretrainedlstm(x).squeeze()).cpu()
